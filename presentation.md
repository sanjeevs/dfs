Slide High Throughput Packet Processing Model
---------------------------------------
+ Input block sprays on an array of processing cores.
+ The cores perform stateless processing.
+ The output block reorder the packets on a flow basis.

> This is a great way to do high throughput packet processing.
  The important thing to remember from the slide is

+ All cores are identical.

> Easy to see that this would be true since the input block does not know
  what type of packet it is and so treats all cores as the same.
   
+ Each core works on 1 packet for its entire lifetime.

> Easy to see since all cores are identical there seem little incentive to 
  use multiple cores.

Slide But sometimes 
--------------------------
+ We need a hetrogeneous mixture of cores.
+ A common example is an edge processor.
    * Need switching/routing.
    * Need encrypting/decryption.
    * Need Firewall/IDS
    ...

> What if the processing is so rich that we need different types of hardware
  cores ? Things like security, regular_expressions ..etc.
  In such cases a simpler hardware can outperform a high end generic core.
  But the traffic requirement is such that we dont want to add these hardware
  accelarators to each core.

Slide Dynamic Flow Scheduler
----------------------------
+ Dynamic flow scheduler purpose is to enable flexible packet processing on 
  an array of **hetrogeneous** cores.

> Hetrogeneous means that cores have different capibilities. Some are soft 
  cores and some are hardwired for certain processing only.

Slide Rework the software model
--------------------------------
+ Packet processing is **not** a giant blob of code.

> Previous model viewed the software as a black box. Hw did not concern itself
  with what the software was doing.
  This is great for high throughput as it decouples the hardware and software.

+ Our view "Packet processing is a *Directed Acyclic Graph* of tasks executing".

+ A packet flow is a unique path through this DAG.

> This will become clear once we know what a task is and so lets define
  some terms.

Slide Our definitions 
----------------------
+ A task is a unit of processing.
    * It has some inputs.
    * Once the inputs are known the task can execute.
        + As it executes other input variables get known.
        + This causes new tasks to run.
    * Once finished this task will not get executed again. 

> Good example of tasks are things like say "L2 Processing". Once the first 'n'
  bytes of the packet are known then then the l2 processing task will run.
  Depending on the packet it might cause the "L3 processing" to run.

+ A flow always goes through the same sequence of tasks.
    * An L2 flow will execute L2ProcessingTask
    * An L3 flow will execute L2ProcessingTask --> L3ProcessingTask.

Slide 10K View
----------------------
+ SW programs the DFS with the DAG.
+ DFS schedules **tasks** on the processing core.

> Unlike previous model we do not spray packets on cores. Instead DFS will 
  schedule tasks on various cores. This is key since now the DFS can choose
  the cores depending on the task it needs to execute.

+ The sw running the tasks sends messages to the DFS indicating which inputs
  are now known.

+ This causes DFS to trace through the DAG and invoke new tasks till the 
  processing is complete.

Slide A possible implementation
---------------------------------
+ DFS implements the DAG as a table.
    * Initilized by SW and readonly for HW.
    * Each entry in the DAG table is a task.
    * The task has 1 set for each input it needs. 0 if it is not required.
+  DFS maintains a context table.
    * Each entry correspond to a packet being processed.
    * The row has 1 set for each input that is known. 0 if unknown.

> For ease of implementation we have kept the bit position of inputs in the 
  DAG table and the context table same. Hence a simple 'and' will tell whether
  the task is ready to be run or not.
  
+ DFS maintains a internal Task History table.
    * Used for ensuring that a task runs only once for a packet context.
    
Slide Scheduler Algorithm
--------------------------
+ Compute the list of ready tasks for each packet context.
    * Foreach packet context entry
        * Search the DAG table for tasks that can run.
        * Skip the tasks that have already run.
+ Launch all the ready tasks

Slide Message Protocol
---------------------------
+ DFS -> CoreHW
    * runTask(ctxt_id, task_id)

+ CoreSW -> DFS
     * setMsg(ctxt_id, var_id, ......)
     * resultMsg(ctxt_id, result_id)

> These are generated by sw. They are asynchronous and non blocking.
  HW makes no assumption on the sender and order of messages.

Slide Summary
-------------------
Pros
-----
+ Ability to have multiple hetrogeneous cores working on the same packet. 
+ Fine grained flow scheduling. 
    * DFS knows where this packet is in the DAG and hence can prioritize.

Cons
----
+ Created more memory traffic since the tasks use shared memory to communicate.
+ DFS processing adds latency to the path.
